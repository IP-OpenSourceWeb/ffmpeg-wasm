import { ffmpegGitUrl, ffmpegPath } from '../../../packages/constants.js';
import { getAllTags, getLatestTag, gitClone } from '../git.functions.js';

const allTags = await getAllTags(ffmpegGitUrl);

if (allTags) {
  const latestFfmpegReleaseTag = getLatestTag(allTags);
  console.log(latestFfmpegReleaseTag);
  if (latestFfmpegReleaseTag) {
    await gitClone(ffmpegGitUrl, latestFfmpegReleaseTag, ffmpegPath);
  }
}

// Options - make script to generate scripts.json file that would get automatically updated with the paths and everything - e.g. then we run everything through scripts like git-clone of paths instead of having a script for each one of them, but that has less customizability from the script and everythign has to be done using the cli args
// 2 - per project make a different script and just run them all in parallel
// I like more option 2 - so , make an array  that contains all the libs that need to be generated, that should include then the gitUrl, path to clone, and then some versions.json where we save what version we downloaded.
// all of the autogenerated files will be then kept in the repo (maybe), maybe have a version.json for each lib, idk,
// then run the script to autogenerate a package.json that would contain everything,
// we need to regenerate them only when adding a new lib.
// rethink if this is better than manual management...
